{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3c99d9a-fdcd-4dff-a16c-41c327e01f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "c38e5d5d-8a23-428f-aaf3-4fd22f70bb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.eeg_datasets import inner_speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "c1c5727f-2830-44a4-b849-b14adc63cbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_speech = reload(inner_speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "452725e9-5bf7-4b48-b435-31e13440bcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import v2\n",
    "from data.transforms.transform import TransposeTransform, EEGFourierTransform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "33b62815-0425-452e-b95e-5668042e837f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "21a8d664-6c39-4cd0-abf3-4fd4ebd1b60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = v2.Compose([\n",
    "    # EEGFourierTransform(),\n",
    "    TransposeTransform(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "75a94e37-2903-4fcc-b71d-5536853b7a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = inner_speech.BIDSEEGDataset('/media/elijah/T7/ds000117-download', transform=TransposeTransform())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "8d005df8",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "data_path = '/media/elijah/T7/ds000117-download'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749f8fde-aa8d-4066-9b81-36da298df960",
   "metadata": {},
   "source": [
    "## Dataset conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5057c87f",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "dedbcb81",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# schema = {'eeg': 'ndarray:float32:74,2048', 'label':'ndarray:int32:2048', 'subject':'str'}\n",
    "# import os\n",
    "# import mne\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from tqdm.autonotebook import tqdm\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from streaming import MDSWriter\n",
    "\n",
    "# def load_file_info(bids_path):\n",
    "#     file_data = []\n",
    "#     for subject_dir in os.listdir(bids_path):\n",
    "#         full_subject_dir = os.path.join(bids_path, subject_dir)\n",
    "#         if subject_dir.startswith('sub-'):\n",
    "#             for session_dir in os.listdir(full_subject_dir):\n",
    "#                 if session_dir.startswith('ses-meg'):\n",
    "#                     meg_dir = os.path.join(full_subject_dir, session_dir, 'meg')\n",
    "#                     for file in os.listdir(meg_dir):\n",
    "#                         if file.endswith('_meg.fif'):\n",
    "#                             fif_path = os.path.join(meg_dir, file)\n",
    "#                             events_path = fif_path.replace('_meg.fif', '_events.tsv')\n",
    "#                             if os.path.exists(events_path):\n",
    "#                                 file_data.append({'subject': subject_dir.split('-')[-1], 'fif_path': fif_path, 'events_path': events_path})\n",
    "#     return pd.DataFrame(file_data)\n",
    "\n",
    "# def load_subject_data(events_path, encoder):\n",
    "#     events_df = pd.read_csv(events_path, sep='\\t')\n",
    "#     session_data = []\n",
    "#     for idx, event in events_df.iterrows():\n",
    "#         session_data.append({\n",
    "#             'start': event['onset'],\n",
    "#             'duration': event['duration'] if 'duration' in event else 1.0,  # Assuming default duration as 1.0 if not provided\n",
    "#             'label': event['stim_type'],\n",
    "#         })\n",
    "#     df = pd.DataFrame(session_data)\n",
    "#     df['label'] = encoder.transform(df['label'])\n",
    "#     df['duration'] = np.diff(df['start'], append=df['start'].iloc[-1] + 1)\n",
    "\n",
    "#     return df\n",
    "\n",
    "# def create_label_vector(events, sfreq, start_sample, segment_length):\n",
    "#     label_vector = np.zeros(segment_length, dtype=int)\n",
    "#     for event in events:\n",
    "#         event_start_sample = int(event['start'] * sfreq)\n",
    "#         event_end_sample = event_start_sample + int(event['duration'] * sfreq)\n",
    "#         event_label = event['label']\n",
    "        \n",
    "#         for i in range(segment_length):\n",
    "#             sample_index = start_sample + i\n",
    "#             if event_start_sample <= sample_index < event_end_sample:\n",
    "#                 label_vector[i] = event_label\n",
    "                \n",
    "#     return label_vector\n",
    "\n",
    "# def preprocess_file(file_info, encoder, fixed_length=2048, resample_rate=None, transform=None):\n",
    "#     raw = mne.io.read_raw_fif(file_info['fif_path'], preload=True, verbose=False)\n",
    "#     raw.pick_types(eeg=True, meg=False, verbose=False)\n",
    "\n",
    "#     if resample_rate:\n",
    "#         raw.resample(resample_rate)\n",
    "\n",
    "#     sfreq = raw.info['sfreq']\n",
    "#     total_samples = raw.n_times\n",
    "#     subject = file_info['subject']\n",
    "\n",
    "#     events = load_subject_data(file_info['events_path'], encoder).to_dict('records')\n",
    "#     # yield events\n",
    "\n",
    "#     for start in range(0, total_samples, fixed_length):\n",
    "#         end = start + fixed_length\n",
    "#         if end > total_samples:\n",
    "#             break  # Ignore the last segment if it's smaller than the fixed length\n",
    "\n",
    "#         eeg_segment = raw.get_data(start=start, stop=end)\n",
    "#         if transform:\n",
    "#             eeg_segment = transform(eeg_segment)\n",
    "\n",
    "\n",
    "#         label_vector = create_label_vector(events, sfreq, start, fixed_length)\n",
    "\n",
    "#         yield {\n",
    "#             'eeg': eeg_segment.astype(np.float32),\n",
    "#             'label': label_vector.astype(np.int32),\n",
    "#             'subject': subject\n",
    "#         }\n",
    "\n",
    "# def fit_label_encoder(bids_path):\n",
    "#     all_labels = []\n",
    "#     for subject_dir in os.listdir(bids_path):\n",
    "#         full_subject_dir = os.path.join(bids_path, subject_dir)\n",
    "#         if subject_dir.startswith('sub-'):\n",
    "#             for session_dir in os.listdir(full_subject_dir):\n",
    "#                 if session_dir.startswith('ses-meg'):\n",
    "#                     meg_dir = os.path.join(full_subject_dir, session_dir, 'meg')\n",
    "#                     for file in os.listdir(meg_dir):\n",
    "#                         if file.endswith('_meg.fif'):\n",
    "#                             events_path = os.path.join(meg_dir, file.replace('_meg.fif', '_events.tsv'))\n",
    "#                             if os.path.exists(events_path):\n",
    "#                                 events_df = pd.read_csv(events_path, sep='\\t')\n",
    "#                                 all_labels.extend(events_df['stim_type'].values)\n",
    "#     encoder = LabelEncoder()\n",
    "#     encoder.fit(all_labels)\n",
    "#     return encoder\n",
    "\n",
    "# def data_generator(bids_path, fixed_length=2048, resample_rate=None, transform=None):\n",
    "#     encoder = fit_label_encoder(bids_path)\n",
    "#     print(encoder.classes_)\n",
    "#     file_info_df = load_file_info(bids_path)\n",
    "\n",
    "#     for idx in tqdm(range(len(file_info_df))):\n",
    "#         file_info = file_info_df.iloc[idx]\n",
    "    \n",
    "#         segments = preprocess_file(file_info, encoder, fixed_length=fixed_length, resample_rate=resample_rate, transform=transform)\n",
    "#         if segments:\n",
    "#             for segment in segments:\n",
    "#                 yield segment\n",
    "\n",
    "\n",
    "# bids_path = data_path\n",
    "output_path = '/media/elijah/T7/ds117_streaming_eeg/'\n",
    "# with MDSWriter(out=output_path, columns=schema) as out:\n",
    "#     for data in data_generator(bids_path, fixed_length=2048, resample_rate=256):\n",
    "#         # print(data)\n",
    "#         # if not (data['label']==data['label'][0]).all():\n",
    "#         # print(data)\n",
    "#         out.write(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "f33e7cd7",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# !rm -r /media/elijah/T7/ds117_streaming_eeg/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "58c511a8-5d7c-4b55-b903-4c87409a6c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import random_split, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8463026-38cd-48da-b75b-ab83f0498d6c",
   "metadata": {},
   "source": [
    "## Model description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "4a2ae2fc-65c6-425b-8d8f-d7ecdb3a42c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# self.accuracy = Accuracy(task='multiclass',num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca55409-dbf2-4da0-8ad4-436f80dda942",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "25d3f8df-bb52-411e-905a-fb25aee94343",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Because `predownload` was not specified, it will default to 8*batch_size if batch_size is not None, otherwise 64. Prior to Streaming v0.7.0, `predownload` defaulted to max(batch_size, 256 * batch_size // num_canonical_nodes).\n"
     ]
    }
   ],
   "source": [
    "from streaming import StreamingDataset, StreamingDataLoader\n",
    "batch_size = 32\n",
    "dataset = StreamingDataset(local=output_path, batch_size=batch_size)\n",
    "loader = StreamingDataLoader(dataset,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "fd1d2a81-7fa9-47ff-af2f-1731fff9f372",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eeg': array([[4.5864883e-05, 2.8982113e-05, 3.1193646e-05, ..., 2.8771536e-05,\n",
       "         3.1445194e-05, 3.3458284e-05],\n",
       "        [3.0096837e-05, 2.9869638e-05, 2.9401632e-05, ..., 3.2504009e-05,\n",
       "         3.4547022e-05, 2.8847438e-05],\n",
       "        [4.5079796e-05, 4.6390494e-05, 4.2576165e-05, ..., 3.4838755e-05,\n",
       "         3.2985994e-05, 2.8013463e-05],\n",
       "        ...,\n",
       "        [5.1696119e-05, 7.9748243e-05, 8.5277134e-05, ..., 6.6001972e-05,\n",
       "         4.1906096e-05, 1.7044764e-05],\n",
       "        [3.1810272e-05, 3.7320398e-05, 4.5373698e-05, ..., 4.0132742e-05,\n",
       "         4.0790146e-05, 4.0565988e-05],\n",
       "        [5.0761835e-05, 5.3283475e-05, 5.2247789e-05, ..., 3.6617876e-05,\n",
       "         3.1128166e-05, 2.3539398e-05]], dtype=float32),\n",
       " 'label': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0], dtype=int32),\n",
       " 'subject': '01'}"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "74a788ff-dadf-4bc2-8388-b0fc5d1184a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 74, 1)"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[3]['eeg'][None, ...].T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "48a89c25-1f1c-4225-8267-ff57f94279bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 74, 129, 17])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "def compute_spectrograms(eeg_data, fs=256, n_fft=256, hop_length=128):\n",
    "    \"\"\"\n",
    "    Compute the spectrogram for each channel of the EEG data using PyTorch.\n",
    "\n",
    "    Parameters:\n",
    "    eeg_data (torch.Tensor): Input EEG data with shape (batch_size, sequence_length, num_channels)\n",
    "    fs (int): Sampling frequency, default is 256 Hz\n",
    "    n_fft (int): Number of FFT components, default is 256\n",
    "    hop_length (int): Number of samples between successive frames, default is 128\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: Spectrogram images with shape (batch_size, num_channels, freq_bins, time_bins)\n",
    "    \"\"\"\n",
    "    batch_size, sequence_length, num_channels = eeg_data.shape\n",
    "    spectrogram_images = []\n",
    "\n",
    "    for batch_idx in range(batch_size):\n",
    "        spectrograms = []\n",
    "        for channel_idx in range(num_channels):\n",
    "            # Compute the spectrogram using torchaudio\n",
    "            channel_data = eeg_data[batch_idx, :, channel_idx]\n",
    "            spec = torchaudio.transforms.Spectrogram(n_fft=n_fft, hop_length=hop_length)(channel_data)\n",
    "            spectrograms.append(spec)\n",
    "        spectrogram_images.append(torch.stack(spectrograms, dim=0))\n",
    "\n",
    "    return torch.stack(spectrogram_images, dim=0)\n",
    "\n",
    "# Example usage\n",
    "eeg_data = torch.randn(10, 2048, 74)  # 10 samples, each with 2048 sequence length and 74 channels\n",
    "spectrograms = compute_spectrograms(eeg_data)\n",
    "\n",
    "print(spectrograms.shape)  # Should print (10, 74, freq_bins, time_bins)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "c6b329e8-15fd-4bc0-9b01-edcedb7d9b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Because `num_canonical_nodes` was not specified, and `shuffle_algo` is py1e, it will default to be equal to physical nodes. Prior to Streaming v0.7.0, `num_canonical_nodes` defaulted to 64 * physical nodes.\n",
      "Because `shuffle_block_size` was not specified, it will default to max(4_000_000 // num_canonical_nodes, 1 << 18) if num_canonical_nodes is not None, otherwise 262144. Prior to Streaming v0.7.0, `shuffle_block_size` defaulted to 262144.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 512, 74])"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(loader))['eeg'].permute(0, 2, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "d39aff2a-c712-4eba-9c2f-81eebe4bb086",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 512])"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(loader))['label'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "5f27363a-f3d3-4e0e-b698-833042735e3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'512'.isdigit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "404bbfc6-fd1d-497e-8f4b-1c5cd4791f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch import nn\n",
    "# import pytorch_lightning as pl\n",
    "# import torch.nn.functional as F\n",
    "# import torchmetrics\n",
    "\n",
    "# class EEGStepClassifier(pl.LightningModule):\n",
    "#     def __init__(self, num_channels=74, d_model=512, num_classes=8, num_layers=3):\n",
    "#         super().__init__()\n",
    "#         self.save_hyperparameters()\n",
    "#         self.ignore_index = -1\n",
    "#         self.lstm = nn.LSTM(input_size=num_channels, hidden_size=d_model, batch_first=True, num_layers=num_layers, dropout=0.1)\n",
    "#         self.embedding = nn.Embedding(num_embeddings=17, embedding_dim=d_model)\n",
    "#         self.fc2 = nn.Linear(d_model * 2, d_model)  # Adjusted for concatenated embedding\n",
    "#         self.gelu = nn.GELU()\n",
    "#         self.mlp = nn.Sequential(\n",
    "#             nn.Linear(d_model, d_model),\n",
    "#             nn.GELU(),\n",
    "#             nn.Linear(d_model, d_model),\n",
    "#             nn.GELU(),\n",
    "#         )\n",
    "#         self.classifier = nn.Linear(d_model, num_classes)\n",
    "        \n",
    "#         self.accuracy = torchmetrics.Accuracy(task='multiclass', num_classes=num_classes)\n",
    "\n",
    "#     def forward(self, x, id):\n",
    "#         ids = torch.tensor([self.get_id(iid) for iid in id], device=self.device)\n",
    "#         x, (hn, cn) = self.lstm(x)\n",
    "        \n",
    "#         # Get the user embeddings\n",
    "#         user_emb = self.embedding(ids)\n",
    "        \n",
    "#         # Expand user embeddings to match the sequence length\n",
    "#         # x.shape[1] provides the sequence length dynamically\n",
    "#         user_emb = user_emb.unsqueeze(1).expand(-1, x.shape[1], -1)\n",
    "        \n",
    "#         # Concatenate along the feature dimension\n",
    "#         x = torch.cat((x, user_emb), dim=-1)\n",
    "        \n",
    "#         x = self.gelu(self.fc2(x))\n",
    "#         x = self.mlp(x)\n",
    "#         logits = self.classifier(x)\n",
    "#         return logits\n",
    "\n",
    "\n",
    "#     def get_id(self, name: str):\n",
    "#         return int(name) if name.isdigit() else 0\n",
    "\n",
    "#     def training_step(self, batch, batch_idx):\n",
    "#         x = batch['eeg'].permute(0, 2, 1)\n",
    "#         y = batch['label'].view(-1).long()\n",
    "#         id = batch['subject']\n",
    "#         logits = self.forward(x, id)\n",
    "#         loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y, ignore_index=self.ignore_index)\n",
    "#         mask = y != self.ignore_index\n",
    "#         acc = self.accuracy(logits.view(-1, logits.size(-1))[mask], y[mask])\n",
    "#         self.log('train_loss', loss, prog_bar=True, on_step=True, on_epoch=True)\n",
    "#         self.log('train_acc', acc, prog_bar=True, on_step=False, on_epoch=True)\n",
    "#         return loss\n",
    "\n",
    "#     def validation_step(self, batch, batch_idx):\n",
    "#         x = batch['eeg'].transpose(0, 2, 1)\n",
    "#         y = batch['label'].view(-1)\n",
    "#         id = batch['subject']\n",
    "#         logits = self.forward(x, id)\n",
    "#         loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y, ignore_index=self.ignore_index)\n",
    "#         mask = y != self.ignore_index\n",
    "#         acc = self.accuracy(logits.view(-1, logits.size(-1))[mask], y[mask])\n",
    "#         self.log('val_loss', loss, prog_bar=True, on_epoch=True)\n",
    "#         self.log('val_acc', acc, prog_bar=True, on_epoch=True)\n",
    "\n",
    "#     def configure_optimizers(self):\n",
    "#         optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "#         scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3 )\n",
    "#         return [optimizer], [scheduler]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "ace704d5-f24b-481b-8c04-38ff34d5f472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch import nn\n",
    "# import pytorch_lightning as pl\n",
    "# import torch.nn.functional as F\n",
    "# import torchmetrics\n",
    "\n",
    "# class EEGTransformerClassifier(pl.LightningModule):\n",
    "#     def __init__(self, num_channels=74, d_model=512, num_classes=8, num_layers=3):\n",
    "#         super().__init__()\n",
    "#         self.in_norm = nn.InstanceNorm1d(num_features=num_channels)\n",
    "#         self.save_hyperparameters()\n",
    "#         self.ignore_index = -1\n",
    "\n",
    "#         self.transformer = nn.TransformerEncoder(\n",
    "#             nn.TransformerEncoderLayer(\n",
    "#                 d_model=d_model, \n",
    "#                 nhead=8, \n",
    "#                 dim_feedforward=d_model, \n",
    "#                 dropout=0.1\n",
    "#             ),\n",
    "#             num_layers=num_layers\n",
    "#         )\n",
    "#         self.fc1 = nn.Linear(num_channels, d_model)\n",
    "#         self.fc2 = nn.Linear(d_model, num_classes)\n",
    "#         self.accuracy = torchmetrics.Accuracy(task='multiclass', num_classes=num_classes)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.in_norm(x.permute(0, 2, 1))\n",
    "#         x = self.fc1(x)  # Linearly project input to match d_model\n",
    "#         x = x.permute(1, 0, 2)  # Transformer expects (seq_len, batch_size, features)\n",
    "#         x = self.transformer(x)\n",
    "#         x = x.permute(1, 0, 2)  # Back to (batch_size, seq_len, features)\n",
    "#         logits = self.fc2(x)\n",
    "#         return logits\n",
    "\n",
    "#     def training_step(self, batch, batch_idx):\n",
    "#         x, y = batch['eeg'], batch['label'].view(-1).long()\n",
    "#         logits = self.forward(x)\n",
    "#         loss = F.cross_entropy(logits.view(-1, self.hparams.num_classes), y, ignore_index=self.ignore_index)\n",
    "#         acc = self.accuracy(logits.view(-1, self.hparams.num_classes), y)\n",
    "#         self.log('train_loss', loss, prog_bar=True, on_step=True, on_epoch=True)\n",
    "#         self.log('train_acc', acc, prog_bar=True, on_step=False, on_epoch=True)\n",
    "#         return loss\n",
    "\n",
    "#     def validation_step(self, batch, batch_idx):\n",
    "#         x, y = batch['eeg'], batch['label'].view(-1)\n",
    "#         logits = self.forward(x)\n",
    "#         loss = F.cross_entropy(logits.view(-1, self.hparams.num_classes), y, ignore_index=self.ignore_index)\n",
    "#         acc = self.accuracy(logits.view(-1, self.hparams.num_classes), y)\n",
    "#         self.log('val_loss', loss, prog_bar=True, on_epoch=True)\n",
    "#         self.log('val_acc', acc, prog_bar=True, on_epoch=True)\n",
    "\n",
    "#     def configure_optimizers(self):\n",
    "#         optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "#         # scheduler = {\n",
    "#         #     'scheduler': torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3),\n",
    "#         #     'monitor': 'train_loss'  # Make sure this metric is being logged in your validation_step\n",
    "#         # }\n",
    "#         scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.5)\n",
    "#         return [optimizer], [scheduler]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "dde0a6e3-533d-40c7-b132-e3e08f8a4fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "220b2199-b3c1-43f9-8aa6-0163c81fa943",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "de36a343-10df-43c5-b994-4b0924b1bf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics\n",
    "import torchaudio\n",
    "from scipy.signal import butter, lfilter\n",
    "\n",
    "class EEGStepClassifier(pl.LightningModule):\n",
    "    def __init__(self, num_channels=74, d_model=512, num_classes=8, fs=256, n_fft=256, hop_length=128):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.ignore_index = -1\n",
    "        self.fs = fs\n",
    "        self.a = 1.1\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        self.embedding = nn.Embedding(num_embeddings=17, embedding_dim=d_model)\n",
    "        \n",
    "        # Convolutional layers for encoding spectrograms with residual connections\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=num_channels, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2,1))\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.fc1 = nn.Linear(4096, d_model)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(2*d_model, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5)\n",
    "        )\n",
    "        self.classifier = nn.Linear(d_model, num_classes)\n",
    "        \n",
    "        self.accuracy = torchmetrics.Accuracy(task='multiclass', num_classes=num_classes)\n",
    "\n",
    "    def bandpass_filter(self, data, lowcut, highcut, fs, order=5):\n",
    "        nyquist = 0.5 * fs\n",
    "        low = lowcut / nyquist\n",
    "        high = highcut / nyquist\n",
    "        b, a = butter(order, [low, high], btype='band')\n",
    "        return lfilter(b, a, data)\n",
    "\n",
    "    def compute_spectrograms(self, eeg_data):\n",
    "        batch_size, sequence_length, num_channels = eeg_data.shape\n",
    "        spectrogram_images = []\n",
    "\n",
    "        for batch_idx in range(batch_size):\n",
    "            spectrograms = []\n",
    "            for channel_idx in range(num_channels):\n",
    "                # Compute the spectrogram using torchaudio\n",
    "                channel_data = eeg_data[batch_idx, :, channel_idx]\n",
    "                spec_transform = torchaudio.transforms.Spectrogram(n_fft=self.n_fft, hop_length=self.hop_length).to(eeg_data.device)\n",
    "                spec = spec_transform(channel_data)\n",
    "                spectrograms.append(spec)\n",
    "            spectrogram_images.append(torch.stack(spectrograms, dim=0))\n",
    "\n",
    "        return torch.stack(spectrogram_images, dim=0)\n",
    "\n",
    "    def forward(self, x, id):\n",
    "        # Band-pass filter for EEG data (e.g., focusing on 0.5-100 Hz range)\n",
    "        x = torch.tensor([self.bandpass_filter(channel.cpu().numpy(), 0.5, 100, self.fs) for channel in x.permute(0, 2, 1)], device=x.device).permute(0, 2, 1).float()\n",
    "        \n",
    "        # Compute spectrograms\n",
    "        x = self.compute_spectrograms(x)\n",
    "        \n",
    "        # Apply convolutional layers with residual connections\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        \n",
    "        # Flatten and prepare for MLP\n",
    "        batch_size, num_channels, freq_bins, time_bins = x.shape\n",
    "        x = x.view(batch_size, -1)\n",
    "        \n",
    "        ids = torch.tensor([self.get_id(iid) for iid in id], device=self.device)\n",
    "        user_emb = self.embedding(ids)\n",
    "        \n",
    "        x = self.dropout(self.fc1(x))\n",
    "        x = torch.cat((x, user_emb), dim=-1)\n",
    "        x = self.mlp(x)\n",
    "        logits = self.classifier(x)\n",
    "        return logits\n",
    "\n",
    "    def get_id(self, name: str):\n",
    "        return int(name) if name.isdigit() else 0\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x = batch['eeg'].permute(0, 2, 1)\n",
    "        y = batch['label'][..., -1].long()\n",
    "        id = batch['subject']\n",
    "        logits = self.forward(x, id)\n",
    "        loss = F.cross_entropy(logits, y, ignore_index=self.ignore_index)\n",
    "        mask = y != self.ignore_index\n",
    "        acc = self.accuracy(logits[mask], y[mask])\n",
    "        # self.log('train_loss', loss/np.log(self.a), prog_bar=True, on_step=True, on_epoch=True)\n",
    "        # self.log('train_acc', acc*np.log(self.a), prog_bar=True, on_step=False, on_epoch=True)\n",
    "        # self.a+=0.002\n",
    "        self.log('train_loss', loss, prog_bar=True, on_step=True, on_epoch=True)\n",
    "        self.log('train_acc', acc, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x = batch['eeg'].permute(0, 2, 1)\n",
    "        y = batch['label'].view(-1)\n",
    "        id = batch['subject']\n",
    "        logits = self.forward(x, id)\n",
    "        loss = F.cross_entropy(logits, y, ignore_index=self.ignore_index)\n",
    "        mask = y != self.ignore_index\n",
    "        acc = self.accuracy(logits[mask], y[mask])\n",
    "        self.log('val_loss', loss, prog_bar=True, on_epoch=True)\n",
    "        self.log('val_acc', acc, prog_bar=True, on_epoch=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=3e-5)\n",
    "        scheduler = {\n",
    "            'scheduler': torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=3, threshold=1e-2),\n",
    "            'monitor': 'train_acc'  # Make sure this metric is being logged in your validation_step\n",
    "        }\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "# Example usage (requires data loader and trainer setup)\n",
    "# trainer = pl.Trainer(max_epochs=10)\n",
    "# model = EEGStepClassifier()\n",
    "# trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "cd42d057-6b78-4188-91a0-a272e4858826",
   "metadata": {},
   "outputs": [],
   "source": [
    "version=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "54387e9d-f504-475e-a425-c2194e12d31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_classes = len(dataset.encoder.classes_)\n",
    "n_classes = 3\n",
    "# model = EEGStepClassifier(num_classes=n_classes, num_layers=2, d_model=256)\n",
    "model = EEGStepClassifier(num_classes=n_classes, d_model=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "817c5707-1725-4609-9cc3-a16e72d1b20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.nn.utils.rnn import pad_sequence\n",
    "# import torch\n",
    "\n",
    "# def custom_collate_fn(batch):\n",
    "#     x, y = zip(*batch)\n",
    "#     x_padded = pad_sequence(x, batch_first=True, padding_value=0)\n",
    "    \n",
    "#     # Handle possible empty y values\n",
    "#     if any(len(label) == 0 for label in y):\n",
    "#         print(\"Warning: Empty label encountered!\")  # Debugging line\n",
    "#         y = [label if len(label) > 0 else torch.tensor([-100]) for label in y]  # Example fallback\n",
    "    \n",
    "#     y_padded = pad_sequence(y, batch_first=True, padding_value=-100)\n",
    "#     return x_padded, y_padded\n",
    "\n",
    "# # When creating your DataLoader, specify the custom collate function\n",
    "# # dataloader = DataLoader(dataset, batch_size=64, collate_fn=custom_collate_fn, pin_memory=True, shuffle=True, num_workers=11)\n",
    "# train_set_size = int(len(dataset) * 0.8)\n",
    "# valid_set_size = len(dataset) - train_set_size\n",
    "\n",
    "# # split the train set into two\n",
    "# seed = torch.Generator().manual_seed(42)\n",
    "\n",
    "# train_set, valid_set = random_split(dataset, [train_set_size, valid_set_size], generator=seed)\n",
    "\n",
    "# train_loader = DataLoader(train_set, batch_size=16, shuffle=True, pin_memory=True, num_workers=11, collate_fn=custom_collate_fn)\n",
    "# test_loader = DataLoader(valid_set, batch_size=16, shuffle=True, pin_memory=True, num_workers=11, collate_fn=custom_collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "b8d91096-b64c-45f4-b491-191f434b68ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "3a77108b-562a-4a43-939f-f09128c7250d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name       | Type               | Params\n",
      "--------------------------------------------------\n",
      "0 | embedding  | Embedding          | 4.4 K \n",
      "1 | conv1      | Sequential         | 42.8 K\n",
      "2 | conv2      | Sequential         | 74.1 K\n",
      "3 | conv3      | Sequential         | 295 K \n",
      "4 | dropout    | Dropout            | 0     \n",
      "5 | fc1        | Linear             | 1.0 M \n",
      "6 | mlp        | Sequential         | 197 K \n",
      "7 | classifier | Linear             | 771   \n",
      "8 | accuracy   | MulticlassAccuracy | 0     \n",
      "--------------------------------------------------\n",
      "1.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.7 M     Total params\n",
      "6.655     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8587779cf3ca46bda484cfcc415c741a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                                   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elijah/.cache/pypoetry/virtualenvs/eeg-mri-analyzer-C0pNHHSC-py3.10/lib/python3.10/site-packages/pytorch_lightning/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 17. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger, WandbLogger\n",
    "import os\n",
    "version+=1\n",
    "# default logger used by trainer (if tensorboard is installed)\n",
    "# logger = TensorBoardLogger(save_dir=os.getcwd(), version=version, name=\"eeg_classification\")\n",
    "logger = WandbLogger(project='diploma', name='diploma_eeg_spectr_conv',)\n",
    "# Initialize a trainer\n",
    "trainer = Trainer(max_epochs=50, log_every_n_steps=1, logger=logger, )  # Adjust max_epochs as needed, set gpus=0 for CPU\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(model, loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7af72eb3-181a-4d92-b648-8d190eec0d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4edacf56-e46a-42df-86f4-9ccde4ffff75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed May 22 21:10:06 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.147.05   Driver Version: 525.147.05   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:01:00.0  On |                  N/A |\n",
      "|  0%   57C    P2    51W / 220W |   1402MiB /  8192MiB |      4%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      2232      G   /usr/lib/xorg/Xorg                262MiB |\n",
      "|    0   N/A  N/A      2376      G   /usr/bin/gnome-shell               48MiB |\n",
      "|    0   N/A  N/A      5878      G   ...349549318063534381,131072      281MiB |\n",
      "|    0   N/A  N/A      6474      G   ...--variations-seed-version       96MiB |\n",
      "|    0   N/A  N/A    166774      G   .../elijah/Telegram/Telegram        3MiB |\n",
      "|    0   N/A  N/A    539524      C   ...0pNHHSC-py3.10/bin/python      704MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ef6f36-94a5-428e-bd4c-f6e8801a8a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kill -9 538114"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732f8024-6ab5-4063-9c21-932cd38db2fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2553009b-52d8-4db9-9616-695c3861a7dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c333229a-b7df-4afd-afea-d285cbaac26d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cac06b8-6bdb-4c99-9ecf-f7a2dd6e57d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9465a1-0522-49cc-a372-d9d507f5f250",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
